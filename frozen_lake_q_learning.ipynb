{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c4cdd57",
   "metadata": {},
   "source": [
    "# Mastering FrozenLake-v1 with Q-Learning ‚õÑ\n",
    "\n",
    "<img src='https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/thumbnail.jpg' alt='Unit 2 Thumbnail'>\n",
    "\n",
    "Welcome to this hands-on tutorial where you'll code your first Reinforcement Learning agent from scratch! We will use the **Q-Learning** algorithm to train an agent to solve the `FrozenLake-v1` environment. By the end, you'll have a trained model, a video of it in action, and you'll even publish it to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba522cc3",
   "metadata": {},
   "source": [
    "### üéØ Project Objectives\n",
    "\n",
    "- **Understand Q-Learning**: Grasp the theory behind Q-tables, actions, states, and rewards.\n",
    "- **Implement from Scratch**: Code the core components of the Q-Learning algorithm using Python and NumPy.\n",
    "- **Use Gymnasium**: Learn to interact with standard RL environments using the Gymnasium library.\n",
    "- **Train & Evaluate**: Run the training loop and evaluate the agent's performance.\n",
    "- **Share Your Work**: Push your trained agent to the Hugging Face Hub with a model card and video replay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9384f",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Installations\n",
    "\n",
    "First, we need to install the necessary libraries. In a cloud environment like Google Colab, we also need to set up a virtual display to render the game and create a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a505cf64",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy gymnasium pygame imageio tqdm pickle5 huggingface_hub pyvirtualdisplay > /dev/null 2>&1\n",
    "!sudo apt-get update > /dev/null 2>&1\n",
    "!sudo apt-get install -y python3-opengl > /dev/null 2>&1\n",
    "!apt install ffmpeg xvfb > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f9a30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "After installation, we start the virtual display. **If you are running this on Google Colab, you might need to restart the runtime after the installations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f620e09",
   "metadata": {},
   "source": [
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377ccad",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Utilities\n",
    "\n",
    "Now, let's import the libraries we'll use throughout the notebook. We also import our custom helper functions from `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import helper functions\n",
    "# If running locally without utils.py, you would define these functions here\n",
    "from utils import evaluate_agent, push_to_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee2cbd7",
   "metadata": {},
   "source": [
    "## Step 3: The Environment - FrozenLake-v1 ‚ùÑÔ∏è\n",
    "\n",
    "Let's create and understand the `FrozenLake-v1` environment.\n",
    "\n",
    "üëâ **Documentation**: [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "\n",
    "The agent's goal is to navigate from the start (S) to the goal (G) on a grid of frozen tiles (F), avoiding holes (H).\n",
    "\n",
    "- `map_name=\"4x4\"`: A 4x4 grid.\n",
    "- `is_slippery=False`: The agent's movement is deterministic (it always moves in the intended direction).\n",
    "- `render_mode=\"rgb_array\"`: Required to capture frames for our video replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b09e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d03bd",
   "metadata": {},
   "source": [
    "### Understanding the State and Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Observation Space (State Space):\", env.observation_space)\n",
    "state_space = env.observation_space.n\n",
    "print(f\"There are {state_space} possible states\\n\")\n",
    "\n",
    "print(\"Action Space:\", env.action_space)\n",
    "action_space = env.action_space.n\n",
    "print(f\"There are {action_space} possible actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c22491",
   "metadata": {},
   "source": [
    "- **State Space**: `Discrete(16)` means there are 16 states, one for each tile on the 4x4 grid.\n",
    "- **Action Space**: `Discrete(4)` means there are 4 possible actions:\n",
    "  - 0: `LEFT`\n",
    "  - 1: `DOWN`\n",
    "  - 2: `RIGHT`\n",
    "  - 3: `UP`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1acd81",
   "metadata": {},
   "source": [
    "## Step 4: Building the Q-Learning Algorithm\n",
    "\n",
    "<img src='https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg' alt='Q-Learning Pseudocode' width='800'/>\n",
    "\n",
    "### 4.1. Initialize the Q-Table\n",
    "The Q-Table stores the expected future rewards for each action in each state. We initialize it with all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df73a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros((state_space, action_space))\n",
    "    return Qtable\n",
    "\n",
    "q_table = initialize_q_table(state_space, action_space)\n",
    "print(\"Q-Table Shape:\", q_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36cee8",
   "metadata": {},
   "source": [
    "### 4.2. Define the Epsilon-Greedy Policy\n",
    "This policy decides the agent's action at each step. It balances between **exploitation** (choosing the best-known action) and **exploration** (choosing a random action) to discover new strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa765e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    if random.uniform(0, 1) > epsilon:\n",
    "        # Exploit: choose the action with the highest Q-value\n",
    "        action = np.argmax(Qtable[state][:])\n",
    "    else:\n",
    "        # Explore: choose a random action\n",
    "        action = env.action_space.sample()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bcd581",
   "metadata": {},
   "source": [
    "## Step 5: Define Hyperparameters and Training Loop\n",
    "\n",
    "Hyperparameters are the settings we provide to the learning algorithm. They can significantly affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 10000\n",
    "learning_rate = 0.7\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"FrozenLake-v1\"\n",
    "max_steps = 99\n",
    "gamma = 0.95\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.05\n",
    "decay_rate = 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ef6da",
   "metadata": {},
   "source": [
    "### 5.1. The Training Function\n",
    "This function implements the main Q-Learning loop described in the pseudocode above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        state, info = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Q-Table update rule\n",
    "            Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            state = new_state\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5c4ae",
   "metadata": {},
   "source": [
    "## Step 6: Train the Agent\n",
    "\n",
    "Now, let's call the `train` function to start training our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39446cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_q_table = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741c88c",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the Agent\n",
    "\n",
    "After training, we evaluate the agent's performance. We use a deterministic policy (no exploration) to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b20abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval_episodes = 100\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, trained_q_table, seed=None)\n",
    "print(f\"Mean Reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a128ce",
   "metadata": {},
   "source": [
    "## Step 8: Publish to Hugging Face Hub üî•\n",
    "\n",
    "Finally, let's share our trained model with the community!\n",
    "\n",
    "1. **Login to your Hugging Face account.** You'll need a token with `write` permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d13a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5168b032",
   "metadata": {},
   "source": [
    "2. **Package the model and hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"env_id\": env_id,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"n_training_episodes\": n_training_episodes,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"eval_seed\": [],\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "    \"qtable\": trained_q_table\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f1a4e",
   "metadata": {},
   "source": [
    "3. **Push to the Hub!**\n",
    "   - Replace `<your-username>` with your Hugging Face username.\n",
    "   - Create a unique repository name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"<your-username>\" # FILL THIS\n",
    "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
    "repo_id = f\"{username}/{repo_name}\"\n",
    "\n",
    "push_to_hub(repo_id, model, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c49d2c0",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You have successfully built, trained, and published a Q-Learning agent. You can now try:\n",
    "\n",
    "- **Experimenting with hyperparameters**: Change `learning_rate`, `gamma`, or the `epsilon` decay schedule.\n",
    "- **Using the slippery version**: Set `is_slippery=True` to make the environment stochastic and more challenging.\n",
    "- **Trying a larger map**: Use `map_name=\"8x8\"`.\n",
    "\n",
    "This project forms a solid foundation for tackling more complex problems in reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
